<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8"/>
        <link href="styles.css" type="text/css" rel="stylesheet">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Pixelify+Sans:wght@400..700&display=swap" rel="stylesheet">
        <title>Portfolio Website</title>       
    </head>
    <body>
        <!--for all links to other website pages (the Home,Cv,Projects,Contact)-->
            <header>
                <ul class="container">
                    <li><a href="./index.html">Home</a></li>
                    <li><a href="./projects.html">Projects</a></li>
                    <li><a href="./cv.html">About</a></li>
                    <li><a href="./contact.html">Contact</a></li>
                </ul>
            </header>
        <div class="dividers">
            <hr class="solid">
        </div>
        <h1>CoSTAR Live Lab</h1>
        <p>I completed a 12 week internship as Marketing and Events Coordinator at CoSTAR Live Lab. During my internship, 
            I had the opportunity to explore different immersive and creative technologies such as motion capture, an L-Acoustics L-ISA system and different face tracking technologies used in research surrounding user experience.</p>
            <br>
            <p>*insert recap video for uni*</p>
            <br>
        <h3>Xtra Reality - Animating Meta Humans using motion capture </h3>
        <p> To assist the technical team with investigating new workflows to improve XR content intended for the magic leap 2,
            I explored and created different Meta Humans using the Meta Human creator plugin for Unreal Engine 5.6. <br>
            <br>
            The Meta Human intended for the final project was designed to replicate the actor Miriam Teek Lee (the original performer for &Juliet).
            <br>
            <br>
            Xtra Reality's team visited CoSTAR Live Lab to first record the motion capture data that would be used to animate the Meta Humans. For the facial motion capture, we used the Live Link app and for the body we used traditional motion capture technology with Vicon cameras.
            <br>
            <br>
            Once the performance was captured, I was resposible for importing the data into Unreal Engine. For the body motion capture, I imported the sequence as an fbx file to create an animtation sequence which was then retargetted in Unreal Engine to fit the skeletal mesh of the intended Meta Human.
            When this was completed, I created an animation blueprint and paired the sequence to the end pose before adding the animation blueprint to the root body skeletal mesh of the Meta Human blueprint.
            <br>
            <br>
            For the facial motion capture, I importeed the Live Link recordings and stored them as capture sources to help create a Meta Human identity from the footage. I tested this workflow by creating my own calibration take and using it to test footage, 
            exporting this as animation sequences from the Meta Human performance.
            <br>
            <br>
            By creating this test, I was able to identify ways to improve our current facial motion capture approaches for cleaner animation.
        </p>
        </nav>
    </body>
</html>